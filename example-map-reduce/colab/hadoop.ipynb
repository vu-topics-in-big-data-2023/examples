{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hadoop",
      "provenance": [],
      "authorship_tag": "ABX9TyNozr7EFR9V0YHjii0jjUJI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vu-topics-in-big-data-2022/examples/blob/main/example-map-reduce/colab/hadoop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8QD8JufDgBB"
      },
      "source": [
        "#install java first\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jm4sRuLzDufI"
      },
      "source": [
        "#install hadoop. But we clean up first \n",
        "!rm -rf hadoop-3.3.0.tar.gz hadoop-3.3.0 /usr/local/hadoop\n",
        "#download\n",
        "!wget https://archive.apache.org/dist/hadoop/common/hadoop-3.3.0/hadoop-3.3.0.tar.gz\n",
        "#untar\n",
        "!tar -xzvf hadoop-3.3.0.tar.gz >/dev/null\n",
        "!mv hadoop-3.3.0 /usr/local/hadoop\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLIB9e_KE8za"
      },
      "source": [
        "#get the data file\n",
        "!wget https://datasets.imdbws.com/title.basics.tsv.gz\n",
        "!gunzip title.basics.tsv.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5ZbiM1nEnMA"
      },
      "source": [
        "#check path and java home\n",
        "!echo $JAVA_HOME\n",
        "os.environ[\"PATH\"]+= os.pathsep + \"/usr/local/hadoop/bin\"\n",
        "!echo $PATH"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSzAtzFJFkap"
      },
      "source": [
        "#check hadoop execution and ensure its in path\n",
        "!/usr/local/hadoop/bin/hadoop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ik9RbcTyt4AF"
      },
      "source": [
        "The hadoop process requires a mapper file and reducer file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1bYn6G2GSoG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bcc8d3a-4c0f-487e-9dcd-cf825de07b49"
      },
      "source": [
        "#the magic command with two %% will take the code in cell and copy to a file.\n",
        "%%file mapper.py\n",
        "#!/usr/bin/python2.7\n",
        "import sys,re\n",
        "\n",
        "def map_function(title):\n",
        "    fields = title.strip().split('\\t')         # Split title to fields using the data delimeter\n",
        "    primaryTitle = fields[2]                   # Select the required field\n",
        "    WORD_RE = re.compile(r\"[\\w']+\")\n",
        "    for word in WORD_RE.findall(primaryTitle):\n",
        "            yield word.strip('!\"*#$&%^').strip(\"'\").lower(), 1\n",
        "    #for word in primaryTitle.strip().split():  # Split primary title by words\n",
        "    #    yield word.strip('!\"*#$&%^').lower(), 1                          # Use a word as a key\n",
        "\n",
        "if __name__ == \"__main__\": \n",
        "  for line in sys.stdin:\n",
        "      # Call map_function for each line in the input\n",
        "      for key, value in map_function(line):\n",
        "          # Emit key-value pairs using '|' as a delimeter  \n",
        "          print(key + \"|\" + str(value))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting mapper.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJGBloFcGlNt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ed44218-ade0-4bbd-df37-c9bb5aa33d9e"
      },
      "source": [
        "%%file reducer.py\n",
        "#!/usr/bin/python2.7\n",
        "import sys\n",
        "\n",
        "prev_word = None\n",
        "value_total = 0\n",
        "\n",
        "if __name__ == \"__main__\": \n",
        "  for line in sys.stdin:          # For ever line in the input from stdin\n",
        "      line = line.strip()         # Remove trailing characters\n",
        "      word, value = line.split(\"|\", 1)\n",
        "      value = int(value)\n",
        "\n",
        "      # Remember that Hadoop sorts mapper output by key, and the reducer takes these keys sorted\n",
        "      if prev_word == word:\n",
        "          value_total += value\n",
        "      else:\n",
        "          if prev_word != None:  # Write result to stdout\n",
        "              print(prev_word + \"|\" + str(value_total))\n",
        "\n",
        "\n",
        "          value_total = value\n",
        "          prev_word = word\n",
        "\n",
        "  if prev_word == word:  # Don't forget the last key/value pair\n",
        "      print(prev_word + \"|\" + str(value_total))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting reducer.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -2 title.basics.tsv"
      ],
      "metadata": {
        "id": "5S3U5rQwd1Mh",
        "outputId": "4f1c1463-52c2-4f0d-bef4-02b86d20bbf7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tconst\ttitleType\tprimaryTitle\toriginalTitle\tisAdult\tstartYear\tendYear\truntimeMinutes\tgenres\n",
            "tt0000001\tshort\tCarmencita\tCarmencita\t0\t1894\t\\N\t1\tDocumentary,Short\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBXsBulVLpC6"
      },
      "source": [
        "#give execute permission and clean output dir\n",
        "!chmod +x mapper.py reducer.py\n",
        "!rm -rf outputdir/"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-IWBXSrGv81"
      },
      "source": [
        "#run hadoop\n",
        "!hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.3.0.jar -input title.basics.tsv -output outputdir -mapper mapper.py -reducer reducer.py -file mapper.py -file reducer.py "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLe3gEm-M5Na",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "988ac542-924c-47f6-aa05-e97660230d0c"
      },
      "source": [
        "# The output will be created in the outputdir - see the -output flag above\n",
        "!ls -l outputdir/\n",
        "!mv outputdir/part-00000 outputdir/hadoop-out.txt"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 10148\n",
            "-rw-r--r-- 1 root root 10388074 Feb 22 17:38 part-00000\n",
            "-rw-r--r-- 1 root root        0 Feb 22 17:38 _SUCCESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVwc-_fxNS5g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "34d4eedb-a0dc-4f35-adfa-2ddcba893a41"
      },
      "source": [
        "#download file\n",
        "from google.colab import files\n",
        "files.download('outputdir/hadoop-out.txt') "
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_afd8b191-97a4-4b4e-99f0-a1ef540bb74b\", \"hadoop-out.txt\", 10388074)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}