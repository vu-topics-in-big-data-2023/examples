{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vu-topics-in-big-data-2023/examples/blob/main/example-spark/pagerank.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4TqXvJnJjdLk"
      },
      "outputs": [],
      "source": [
        "#This is the page rank example taken from spark examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EmHhdiqQho9u"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Exq5o6RiNr2"
      },
      "outputs": [],
      "source": [
        "!wget -q https://downloads.apache.org/spark/spark-3.2.3/spark-3.2.3-bin-hadoop3.2.tgz\n",
        "!tar xf  spark-3.2.3-bin-hadoop3.2.tgz\n",
        "!ls -l\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQiUr_sviyJb"
      },
      "outputs": [],
      "source": [
        "#Provides findspark.init() to make pyspark importable as a regular library.\n",
        "os.environ[\"SPARK_HOME\"] = \"spark-3.2.3-bin-hadoop3.2\"\n",
        "!pip install -q findspark\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "staDohTqjcss"
      },
      "outputs": [],
      "source": [
        "#The entry point to using Spark SQL is an object called SparkSession. It initiates a Spark Application which all the code for that Session will run on\n",
        "# to larn more see https://blog.knoldus.com/spark-why-should-we-use-sparksession/\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"Learning_Spark\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4xisC_gqTt2"
      },
      "source": [
        "# Page Rank example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TFSe_HFqXyt"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/vu-topics-in-big-data-2023/examples/main/example-spark/data/pagerank_data.txt\n",
        "!cat pagerank_data.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVl83W_Mr0-p"
      },
      "outputs": [],
      "source": [
        "lines = spark.read.text(\"pagerank_data.txt\").rdd.map(lambda r: r[0])\n",
        "lines.take(lines.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BVQVvCoq2Svv"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "def parseNeighbors(urls):\n",
        "    \"\"\"Parses a urls pair string into urls pair.\"\"\"\n",
        "    parts = re.split(r'\\s+', urls)\n",
        "    print (parts)\n",
        "    return parts[0], parts[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTXfLOWVs2TK"
      },
      "outputs": [],
      "source": [
        "# Loads all URLs from input file and initialize their neighbors. \n",
        "# Mapping is transforming each RDD element using a function and returning a new RDD\n",
        "links = lines.map(lambda x: parseNeighbors(x))\n",
        "#so finally you get a set of rows, where each row is a tuple where the target is the second element in tuple\n",
        "# and the src is the first element in the tuple. \n",
        "links.take(links.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cs7qHD5YuUAc"
      },
      "outputs": [],
      "source": [
        "neighbors = lines.map(lambda urls: parseNeighbors(urls))\n",
        "neighbors.take(links.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUvBYoaXuorq"
      },
      "outputs": [],
      "source": [
        "#you can see the links as a table as well by printing it as a dataframe\n",
        "neighbors.toDF().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "za4hOHKxd6Zj"
      },
      "outputs": [],
      "source": [
        "# we will try to group the elements by key \n",
        "linksdistinct=lines.map(lambda urls: parseNeighbors(urls)).distinct().groupByKey()\n",
        "#when you print it - you will see that the element's second column is a result iterable.\n",
        "\n",
        "linksdistinct.take(linksdistinct.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwXf-Aj5eLS_"
      },
      "outputs": [],
      "source": [
        "#to see the iterable you can use the following conversion\n",
        "def see_iterable(iterable):\n",
        "    r = []\n",
        "    for v1_iterable in iterable:\n",
        "        for v2 in v1_iterable:\n",
        "            r.append(v2)\n",
        "\n",
        "    return tuple(r)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DaJwFbouVpF4"
      },
      "outputs": [],
      "source": [
        "#lets see the first element now\n",
        "print(linksdistinct.take(1)[0][0]) #this is the key\n",
        "print(linksdistinct.take(1)[0][1]) #this is the iterable\n",
        "print(linksdistinct.take(1)[0][0],see_iterable(linksdistinct.take(1)[0][1]))\n",
        "#you can see that it contains elements 2, 3,4\n",
        "print(linksdistinct.take(4)[1][0],see_iterable(linksdistinct.take(2)[1][1]))\n",
        "print(linksdistinct.take(4)[2][0],see_iterable(linksdistinct.take(3)[2][1]))\n",
        "print(linksdistinct.take(4)[3][0],see_iterable(linksdistinct.take(4)[3][1]))\n",
        "print(\"compare this to. And see what group by did. It brough all pages that brough to one and collected them\")\n",
        "neighbors.toDF().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njHGndftRpo5"
      },
      "outputs": [],
      "source": [
        "#so again ensure linkdistinct is initations\n",
        "linksdistinct=lines.map(lambda urls: parseNeighbors(urls)).distinct().groupByKey()\n",
        "#now we set up a rank matrix using the the 0th element from list of rows: for example linksdistinct.take(4)[1][0] - take all rows, pick first row and pick 0th element, linksdistinct.take(4)[2][0] will take the 0th element from the list that is the second row\n",
        "#thus the following operation will get us a RDD where the rows are 1,2,3,4 and 1.0\n",
        "ranks = linksdistinct.map(lambda url_neighbors: (url_neighbors[0], 1.0))\n",
        " # Loads all URLs with other URL(s) link to from input file and initialize ranks of them to one.\n",
        "print(ranks.collect())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5gNc2RHRMIf"
      },
      "outputs": [],
      "source": [
        "#lets see the join operation. See how it aggregates by key\n",
        "contribs = linksdistinct.join(ranks)\n",
        "contribs.take(linksdistinct.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSiiZPLNgZY2"
      },
      "outputs": [],
      "source": [
        "#again this is a pyspark iterable. So lets us the previous function.\n",
        "for i in range(4):\n",
        "  print(contribs.take(4)[i][0],see_iterable(contribs.take(4)[i][1][0])) #note that the value is a tuple of iterable and 1.0. hence there is one more index here in the value we pass to see_iterable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAzsP5Ckr0CR"
      },
      "outputs": [],
      "source": [
        "#On each iteration, have page p send a contribution of rank(p)/numNeighbors(p) to its neighbors (the pages it has links to).\n",
        "#note if we process 1 ('2', '3', '4')\n",
        "#then the rank of 1/3 is contributed towards the rank of 2,3 and 4\n",
        "#hence if at start rank of 1 is 1.0\n",
        "#then the output of this function will be\n",
        "# 2,1/3\n",
        "# 3,1/3\n",
        "# 4,1/3\n",
        "#note that this is only the first iteration\n",
        "def computeContribs(urls, rank):\n",
        "    \"\"\"Calculates URL contributions to the rank of other URLs.\"\"\"\n",
        "    num_urls = len(urls)    \n",
        "    for url in urls:\n",
        "        yield (url, rank/num_urls)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdHrHx98vfhS"
      },
      "outputs": [],
      "source": [
        "#change iterations number to run the algo for multiple times.\n",
        "iterations=1\n",
        "#reinit rank\n",
        "ranks = linksdistinct.map(lambda url_neighbors: (url_neighbors[0], 1.0))\n",
        "\n",
        "from operator import add\n",
        "# Calculates and updates URL ranks continuously using PageRank algorithm.\n",
        "for iteration in range(iterations):\n",
        "        # Calculates URL contributions to the rank of other URLs.\n",
        "        contribs = linksdistinct.join(ranks).flatMap(lambda url_urls_rank: computeContribs(url_urls_rank[1][0] #this first element is the list for example. ('2', '3', '4') \n",
        "                                                                                           , url_urls_rank[1][1])) #the second element is the rank - at the start it is 1.0\n",
        "        #comment the show method if you run for lot of iterations\n",
        "        contribs.toDF().show(truncate=False)\n",
        "\n",
        "        # Re-calculates URL ranks based on neighbor contributions.\n",
        "        #note that updating the rank by multiplying by 0.85 and adding 0.15 is a heuristic\n",
        "        ranks = contribs.reduceByKey(add).mapValues(lambda rank: rank * 0.85 + 0.15)\n",
        "        #comment the show method if you run for lot of iterations\n",
        "        print(\"rank\")\n",
        "        ranks.toDF().show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJdXlOIfj_oT"
      },
      "outputs": [],
      "source": [
        "#let us now run it 20 times but without prints\n",
        "iterations=20\n",
        "#reinit rank\n",
        "ranks = linksdistinct.map(lambda url_neighbors: (url_neighbors[0], 1.0))\n",
        "# Calculates and updates URL ranks continuously using PageRank algorithm.\n",
        "for iteration in range(iterations):\n",
        "        # Calculates URL contributions to the rank of other URLs.\n",
        "        contribs = linksdistinct.join(ranks).flatMap(lambda url_urls_rank: computeContribs(url_urls_rank[1][0], url_urls_rank[1][1]))\n",
        "        ranks = contribs.reduceByKey(add).mapValues(lambda rank: rank * 0.85 + 0.15)       \n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGqDLR_Kv-Xl"
      },
      "outputs": [],
      "source": [
        "# Collects all URL ranks and dump them to console.\n",
        "#see the lazy operation. it will take longer to execute this cell than the previous cell.\n",
        "for (link, rank) in ranks.collect():\n",
        "   print(\"%s has rank: %s.\" % (link, rank))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "pagerank",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}