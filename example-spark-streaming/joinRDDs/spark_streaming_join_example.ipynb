{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import json\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "\n",
    "'''\n",
    "Join the batting and salaries data for Barry Bonds per year.\n",
    "\n",
    "The output should be the combined CSV string of batting and salaries data (one per year).\n",
    "\n",
    "Use 'join' as the key for the final output in the reducer.\n",
    "\n",
    "E.g:\n",
    "\"join\"  \"bondsba01,1986,1,PIT,NL,113,413,72,92,26,3,16,48,36,7,65,102,2,2,2,2,4,1986,PIT,NL,bondsba01,60000\"\n",
    "\n",
    "Schema:\n",
    "Salaries: yearID\tteamID\tlgID\tplayerID\tsalary\n",
    "Batting: playerID\tyearID\tstint\tteamID\tlgID\tG\tAB\tR\tH\t2B\t3B\tHR\tRBI\tSB\tCS\tBB\tSO\n",
    "\n",
    "Hints: \n",
    "Use split to split the CSV lines (e.g. s = line.split(','))\n",
    "Both files are sent to the mapper. Use the length of the lines to determine which is which.\n",
    "'''\n",
    "if __name__ == '__main__':\n",
    "    # Create Spark context\n",
    "    conf = SparkConf().setAppName(\"4_join\").set(\"spark.streaming.concurrentJobs\", \"2\").set('spark.hadoop.validateOutputSpecs', False)\n",
    "    sc = SparkContext(conf=conf)\n",
    "    sc.setLogLevel(\"WARN\")\n",
    "\n",
    "    # Create Spark Streaming context\n",
    "    ssc = StreamingContext(sparkContext=sc, batchDuration=1)\n",
    "\n",
    "    # Defining the checkpoint directory\n",
    "    ssc.checkpoint(\"/root/tmp\")\n",
    "\n",
    "    # Connect to Kafka and subscribe two topics using Direct Approach (No Receivers)\n",
    "    # Please check: https://spark.apache.org/docs/2.1.0/streaming-kafka-0-8-integration.html \n",
    "    # for more details about Direct Approch.\n",
    "    kafkaStream = KafkaUtils.createDirectStream(ssc=ssc,\n",
    "                                                kafkaParams={\"metadata.broker.list\": '<your ec2 instance public IP>:9092'},\n",
    "                                                topics=['4_join_batting', '4_join_salaries'])\n",
    "    \n",
    "    '''\n",
    "    Steps of joining samples from Salaries and Batting tables:\n",
    "    1. filter samples by playerID: bondsba01\n",
    "    2. determine whether a sample is from Salaries table or Batting table\n",
    "    3. generate (key, value) pairs where yearID is the key\n",
    "\n",
    "    ----- Once the above steps are done, we can start joining samples...\n",
    "    \n",
    "    What does updateStateByKey do?\n",
    "    Return a new \"state\" DStream where the state for each key is updated by applying the given function on the previous state of the key and the new values for the key. This can be used to maintain arbitrary state data for each key.\n",
    "\n",
    "    ---------------------------------------------------------------------\n",
    "    4. Since the order of receiving Salaries and Batting samples are indeterministic, we need to use the updateStateByKey function to maintain and update state for each key(yearID).\n",
    "    5. Apply a sort function to every RDD within the DStream, so that we can get the output sorted by yearID.\n",
    "    '''\n",
    "\n",
    "    # Filter playerID and distinguish salaries and batting samples\n",
    "    def categorize(fields):\n",
    "      \"\"\"\n",
    "      :param fields: each line is seperated to a list of fields by comma\n",
    "      :return: a (key, value) tuple, where key is yearID/None, and value is Salaries/Batting/None\n",
    "      \"\"\"\n",
    "      if len(fields) == 5 and fields[3] == 'bondsba01':  # find Salaries records for bondsba01\n",
    "        return fields[0], ('S', ','.join(fields))\n",
    "      elif fields[0] == 'bondsba01':  # find Batting records for bondsba01\n",
    "        return fields[1], ('B', ','.join(fields))\n",
    "      else:  # for other players, return None, None\n",
    "        return None, None\n",
    "\n",
    "    def updateState(new, old):\n",
    "      \"\"\"\n",
    "      :param new: new state of a given key\n",
    "      :param old: old state of a given key\n",
    "      :return: a (key, value) tuple where key is yearID and value is a combined Salaries + Batting string.\n",
    "      \"\"\"\n",
    "      tmp = None  # use a intermedia variable tmp to manage Salaries and Batting records\n",
    "      if new and old:\n",
    "        new.extend(old)\n",
    "        tmp = new\n",
    "      elif new:\n",
    "        tmp = new\n",
    "      elif old:\n",
    "        tmp = old\n",
    "\n",
    "      # Combine new and old only if they are both not None or empty list.\n",
    "      if tmp and len(tmp) > 1 and type(tmp) is list:\n",
    "        # @todo: determine the order of salaries and batting in the tmp array, then connect them in the order \"salaries + batting\"\n",
    "      else:\n",
    "        return tmp\n",
    "\n",
    "    \"\"\"    \n",
    "    kafkaStream receives samples from kafka broker, then converts them to key-value pair RDD.\n",
    "    Each line in the lambda statement of the map function is a tuple like below:\n",
    "    (None, 'webbsk01,1945,1,DET,AL,118,118,407,43,81,12,2,0,21,8,7,30,35,,0,12,,9,118\\n')\n",
    "    (None, 'webbsk01,1946,1,DET,AL,64,64,169,12,37,1,1,0,17,3,3,9,18,,0,3,,5,64\\n')\n",
    "    In the initial state, Spark set the key to None. \n",
    "    Note this key is for RDD, it has nothing to do with kafka topics or anything else.\n",
    "\n",
    "    Since the categorize function return (None, None) if the playerID is not bondsba01, \n",
    "    so the filter function preserve Salaries and Batting records for bondsba01 in RDD.\n",
    "    It essentially groups tuples that satisfy the given condition as a list. \n",
    "\n",
    "    So the new and old parameters in the updateState function are list or None, like below:\n",
    "    [('S', '1989,PIT,NL,bondsba01,360000')] None\n",
    "    [('B', 'bondsba01,1990,1,PIT,NL,151,151,519,104,156,32,3,33,114,52,13,93,83,15,3,0,6,8,151')] [('S', '1990,PIT,NL, bondsba01,850000')]\n",
    "    \"\"\"\n",
    "    kstream = kafkaStream.map(lambda line: categorize(line[1].strip().split(','))) \\\n",
    "                         .filter(lambda line: line[0]) \\\n",
    "                         .updateStateByKey(updateState) \\\n",
    "                        # @todo: sort RDD data by key(yearID) in ascending order using RDD transformation\n",
    "    \n",
    "    # kstream is a DStream after applying the above operations.\n",
    "    # Then, save key-value pair RDD as a text file.\n",
    "    # kstream.repartition(1).saveAsTextFiles('./4_join/output')\n",
    "    kstream.foreachRDD(lambda rdd: rdd.repartition(1).saveAsTextFile('s3://vandy-bigdata-kzw/hw6/4_join.out'))\n",
    "\n",
    "\n",
    "    # Start the streaming context\n",
    "    ssc.start()\n",
    "    ssc.awaitTerminationOrTimeout(120)\n",
    "    ssc.stop()\n"
   ]
  }
 ]
}